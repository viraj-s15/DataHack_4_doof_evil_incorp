{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ur4LZ8CSApjY"
      },
      "outputs": [],
      "source": [
        "pip install -q transformers accelerate fastapi uvicorn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsq7p6GZAnCA",
        "outputId": "06cadc0c-c626-4385-81ea-19e37ed590b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> I love you.</s>\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"bigscience/mt0-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, torch_dtype=\"auto\", device_map=\"auto\",early_stopping=True)\n",
        "\n",
        "inputs = tokenizer.encode(\"Translate to English: Je tâ€™aime.\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QSfqQxa3AtBf"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tBAyxqIzA9p2"
      },
      "outputs": [],
      "source": [
        "@app.get(\"/translate_text/\")\n",
        "async def translate(user_id:int,text:str,og_lang:str,translated_lang:str):\n",
        "  original_language = og_lang\n",
        "  encoded_input = tokenizer.encode(f\"Translate to {translated_lang}: {text}.\", return_tensors=\"pt\").to(\"cuda\")\n",
        "  output = model.generate(encoded_input)\n",
        "  translated_text = tokenizer.decode(output[0])\n",
        "\n",
        "  return {\"original_text\":text,\"original_language\":original_language,\"translated_language\":translated_lang,\"transated_text\":translated_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_4swzcuRDwTR"
      },
      "outputs": [],
      "source": [
        "@app.get(\"/\")\n",
        "async def translate():\n",
        "  return {\"message\":\"hello\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R90T0hlxCNW5",
        "outputId": "85e79511-50d6-4136-f017-587f8a304c40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-09-21T09:48:15+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://c484-34-125-10-196.ngrok.io\" -> \"http://localhost:8000\">"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "ngrok_tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6c6bAX7DCCi",
        "outputId": "18b0778c-4956-4dfb-a467-4abb5cb54dd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [5551]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     27.107.113.146:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     27.107.113.146:0 - \"GET /translate_text/?user_id=1212&text=hello%20there%20my%20name%20is%20viraj&translated_lang=French&og_lang=english HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
